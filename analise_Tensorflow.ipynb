{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Modules for ML\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17456 entries, 0 to 17455\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Date              17456 non-null  object \n",
      " 1   Streams           17456 non-null  int64  \n",
      " 2   Viewers           17456 non-null  int64  \n",
      " 3   Dominant_streams  17456 non-null  int64  \n",
      " 4   Dominant_ratio    17456 non-null  float64\n",
      " 5   Tags              17456 non-null  object \n",
      " 6   Name              17456 non-null  object \n",
      " 7   Title             17456 non-null  object \n",
      " 8   Beginning         17456 non-null  object \n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('OV2.csv', on_bad_lines='skip', sep=';', )\n",
    "data = pd.DataFrame(data)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17301 entries, 155 to 17455\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   Date              17301 non-null  datetime64[ns]\n",
      " 1   Streams           17301 non-null  int64         \n",
      " 2   Viewers           17301 non-null  int64         \n",
      " 3   Dominant_streams  17301 non-null  int64         \n",
      " 4   Dominant_ratio    17301 non-null  float64       \n",
      " 5   Tags              17301 non-null  object        \n",
      " 6   Name              17301 non-null  object        \n",
      " 7   Title             17301 non-null  object        \n",
      " 8   Beginning         17301 non-null  object        \n",
      " 9   Hour              17301 non-null  int64         \n",
      " 10  Minute            17301 non-null  int64         \n",
      " 11  Weekday           17301 non-null  int64         \n",
      " 12  Moving_streams    17301 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(6), object(4)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y %H:%M:%S', errors='coerce')\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%d.%m.%Y %H:%M:%S', errors='coerce')\n",
    "data = data.dropna()\n",
    "data['Hour'] = data['Date'].dt.hour\n",
    "data['Minute'] = data['Date'].dt.hour\n",
    "data['Weekday'] = data['Date'].dt.weekday\n",
    "data['Moving_streams'] = data['Streams'].rolling(10, min_periods=1).mean()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Hour','Minute','Weekday']]\n",
    "y = data[['Moving_streams']]\n",
    "\n",
    "y = y.replace(',', '.', regex=True)\n",
    "\n",
    "y= np.asarray(y).astype('float32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "433/433 [==============================] - 4s 3ms/step - loss: 8.5761\n",
      "Epoch 2/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 5.1305\n",
      "Epoch 3/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 4.9615\n",
      "Epoch 4/70\n",
      "433/433 [==============================] - 2s 3ms/step - loss: 4.6492\n",
      "Epoch 5/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 4.4993\n",
      "Epoch 6/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 4.3580\n",
      "Epoch 7/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 4.2637\n",
      "Epoch 8/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 4.1248\n",
      "Epoch 9/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 4.1118\n",
      "Epoch 10/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 4.0462\n",
      "Epoch 11/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.9878\n",
      "Epoch 12/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.9730\n",
      "Epoch 13/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.9201\n",
      "Epoch 14/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.8421\n",
      "Epoch 15/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.8272\n",
      "Epoch 16/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.7896\n",
      "Epoch 17/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.7240\n",
      "Epoch 18/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.7150\n",
      "Epoch 19/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.6837\n",
      "Epoch 20/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.6469\n",
      "Epoch 21/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.5990\n",
      "Epoch 22/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.5903\n",
      "Epoch 23/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.5703\n",
      "Epoch 24/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.5823\n",
      "Epoch 25/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.5150\n",
      "Epoch 26/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.5249\n",
      "Epoch 27/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.5205\n",
      "Epoch 28/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.5040\n",
      "Epoch 29/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.5020\n",
      "Epoch 30/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.4413\n",
      "Epoch 31/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.4411\n",
      "Epoch 32/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.4887\n",
      "Epoch 33/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.3983\n",
      "Epoch 34/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.4315\n",
      "Epoch 35/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.3443\n",
      "Epoch 36/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.3565\n",
      "Epoch 37/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.3265\n",
      "Epoch 38/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.3387\n",
      "Epoch 39/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.2860\n",
      "Epoch 40/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.3058\n",
      "Epoch 41/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.2736\n",
      "Epoch 42/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.2613\n",
      "Epoch 43/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.3022\n",
      "Epoch 44/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.2326\n",
      "Epoch 45/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.2680\n",
      "Epoch 46/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.2370\n",
      "Epoch 47/70\n",
      "433/433 [==============================] - 2s 3ms/step - loss: 3.2016\n",
      "Epoch 48/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.2127\n",
      "Epoch 49/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.2027\n",
      "Epoch 50/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.1978\n",
      "Epoch 51/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1932\n",
      "Epoch 52/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1594\n",
      "Epoch 53/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1803\n",
      "Epoch 54/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1497\n",
      "Epoch 55/70\n",
      "433/433 [==============================] - 2s 4ms/step - loss: 3.1429\n",
      "Epoch 56/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1708\n",
      "Epoch 57/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1633\n",
      "Epoch 58/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1675\n",
      "Epoch 59/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1293\n",
      "Epoch 60/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1077\n",
      "Epoch 61/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1374\n",
      "Epoch 62/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1264\n",
      "Epoch 63/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1198\n",
      "Epoch 64/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.0940\n",
      "Epoch 65/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.0790\n",
      "Epoch 66/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1264\n",
      "Epoch 67/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.1024\n",
      "Epoch 68/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.0759\n",
      "Epoch 69/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.0862\n",
      "Epoch 70/70\n",
      "433/433 [==============================] - 1s 3ms/step - loss: 3.0878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2630a632560>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "433/433 [==============================] - 1s 808us/step - loss: 9.8018 - mse: 9.8018 - accuracy: 0.1029\n",
      "Epoch 2/100\n",
      "433/433 [==============================] - 0s 835us/step - loss: 7.0332 - mse: 7.0332 - accuracy: 0.1181\n",
      "Epoch 3/100\n",
      "433/433 [==============================] - 0s 788us/step - loss: 6.1148 - mse: 6.1148 - accuracy: 0.1306\n",
      "Epoch 4/100\n",
      "433/433 [==============================] - 0s 736us/step - loss: 5.6408 - mse: 5.6408 - accuracy: 0.1450\n",
      "Epoch 5/100\n",
      "433/433 [==============================] - 0s 732us/step - loss: 5.4730 - mse: 5.4730 - accuracy: 0.1413\n",
      "Epoch 6/100\n",
      "433/433 [==============================] - 0s 710us/step - loss: 5.5436 - mse: 5.5436 - accuracy: 0.1428\n",
      "Epoch 7/100\n",
      "433/433 [==============================] - 0s 734us/step - loss: 5.3477 - mse: 5.3477 - accuracy: 0.1460\n",
      "Epoch 8/100\n",
      "433/433 [==============================] - 0s 718us/step - loss: 5.2759 - mse: 5.2759 - accuracy: 0.1465\n",
      "Epoch 9/100\n",
      "433/433 [==============================] - 0s 712us/step - loss: 5.2739 - mse: 5.2739 - accuracy: 0.1466\n",
      "Epoch 10/100\n",
      "433/433 [==============================] - 0s 733us/step - loss: 5.1729 - mse: 5.1729 - accuracy: 0.1485\n",
      "Epoch 11/100\n",
      "433/433 [==============================] - 0s 711us/step - loss: 5.1476 - mse: 5.1476 - accuracy: 0.1485\n",
      "Epoch 12/100\n",
      "433/433 [==============================] - 0s 708us/step - loss: 5.1192 - mse: 5.1192 - accuracy: 0.1524\n",
      "Epoch 13/100\n",
      "433/433 [==============================] - 0s 754us/step - loss: 5.0616 - mse: 5.0616 - accuracy: 0.1509\n",
      "Epoch 14/100\n",
      "433/433 [==============================] - 0s 742us/step - loss: 5.0208 - mse: 5.0208 - accuracy: 0.1553\n",
      "Epoch 15/100\n",
      "433/433 [==============================] - 0s 744us/step - loss: 5.0341 - mse: 5.0341 - accuracy: 0.1549\n",
      "Epoch 16/100\n",
      "433/433 [==============================] - 0s 760us/step - loss: 4.9230 - mse: 4.9230 - accuracy: 0.1559\n",
      "Epoch 17/100\n",
      "433/433 [==============================] - 0s 805us/step - loss: 4.9203 - mse: 4.9203 - accuracy: 0.1557\n",
      "Epoch 18/100\n",
      "433/433 [==============================] - 0s 787us/step - loss: 4.8040 - mse: 4.8040 - accuracy: 0.1580\n",
      "Epoch 19/100\n",
      "433/433 [==============================] - 0s 857us/step - loss: 4.7865 - mse: 4.7865 - accuracy: 0.1595\n",
      "Epoch 20/100\n",
      "433/433 [==============================] - 0s 986us/step - loss: 4.7031 - mse: 4.7031 - accuracy: 0.1595\n",
      "Epoch 21/100\n",
      "433/433 [==============================] - 0s 752us/step - loss: 4.6438 - mse: 4.6438 - accuracy: 0.1618\n",
      "Epoch 22/100\n",
      "433/433 [==============================] - 0s 759us/step - loss: 4.6282 - mse: 4.6282 - accuracy: 0.1631\n",
      "Epoch 23/100\n",
      "433/433 [==============================] - 0s 750us/step - loss: 4.5920 - mse: 4.5920 - accuracy: 0.1624\n",
      "Epoch 24/100\n",
      "433/433 [==============================] - 0s 789us/step - loss: 4.6051 - mse: 4.6051 - accuracy: 0.1627\n",
      "Epoch 25/100\n",
      "433/433 [==============================] - 0s 739us/step - loss: 4.4990 - mse: 4.4990 - accuracy: 0.1636\n",
      "Epoch 26/100\n",
      "433/433 [==============================] - 0s 739us/step - loss: 4.4956 - mse: 4.4956 - accuracy: 0.1634\n",
      "Epoch 27/100\n",
      "433/433 [==============================] - 0s 750us/step - loss: 4.4275 - mse: 4.4275 - accuracy: 0.1621\n",
      "Epoch 28/100\n",
      "433/433 [==============================] - 0s 731us/step - loss: 4.3798 - mse: 4.3798 - accuracy: 0.1651\n",
      "Epoch 29/100\n",
      "433/433 [==============================] - 0s 728us/step - loss: 4.3105 - mse: 4.3105 - accuracy: 0.1642\n",
      "Epoch 30/100\n",
      "433/433 [==============================] - 0s 715us/step - loss: 4.3801 - mse: 4.3801 - accuracy: 0.1626\n",
      "Epoch 31/100\n",
      "433/433 [==============================] - 0s 728us/step - loss: 4.2639 - mse: 4.2639 - accuracy: 0.1645\n",
      "Epoch 32/100\n",
      "433/433 [==============================] - 0s 721us/step - loss: 4.2588 - mse: 4.2588 - accuracy: 0.1637\n",
      "Epoch 33/100\n",
      "433/433 [==============================] - 0s 734us/step - loss: 4.2020 - mse: 4.2020 - accuracy: 0.1628\n",
      "Epoch 34/100\n",
      "433/433 [==============================] - 0s 739us/step - loss: 4.1654 - mse: 4.1654 - accuracy: 0.1639\n",
      "Epoch 35/100\n",
      "433/433 [==============================] - 0s 765us/step - loss: 4.0982 - mse: 4.0982 - accuracy: 0.1646\n",
      "Epoch 36/100\n",
      "433/433 [==============================] - 0s 791us/step - loss: 4.0662 - mse: 4.0662 - accuracy: 0.1595\n",
      "Epoch 37/100\n",
      "433/433 [==============================] - 0s 840us/step - loss: 3.9984 - mse: 3.9984 - accuracy: 0.1639\n",
      "Epoch 38/100\n",
      "433/433 [==============================] - 0s 901us/step - loss: 4.0306 - mse: 4.0306 - accuracy: 0.1618\n",
      "Epoch 39/100\n",
      "433/433 [==============================] - 0s 828us/step - loss: 3.9646 - mse: 3.9646 - accuracy: 0.1628\n",
      "Epoch 40/100\n",
      "433/433 [==============================] - 0s 795us/step - loss: 3.8922 - mse: 3.8922 - accuracy: 0.1607\n",
      "Epoch 41/100\n",
      "433/433 [==============================] - 0s 777us/step - loss: 3.9113 - mse: 3.9113 - accuracy: 0.1613\n",
      "Epoch 42/100\n",
      "433/433 [==============================] - 0s 847us/step - loss: 3.8616 - mse: 3.8616 - accuracy: 0.1650\n",
      "Epoch 43/100\n",
      "433/433 [==============================] - 0s 831us/step - loss: 3.8294 - mse: 3.8294 - accuracy: 0.1592\n",
      "Epoch 44/100\n",
      "433/433 [==============================] - 0s 886us/step - loss: 3.7960 - mse: 3.7960 - accuracy: 0.1624\n",
      "Epoch 45/100\n",
      "433/433 [==============================] - 0s 898us/step - loss: 3.7839 - mse: 3.7839 - accuracy: 0.1654\n",
      "Epoch 46/100\n",
      "433/433 [==============================] - 0s 1ms/step - loss: 3.7365 - mse: 3.7365 - accuracy: 0.1629\n",
      "Epoch 47/100\n",
      "433/433 [==============================] - 0s 955us/step - loss: 3.7272 - mse: 3.7272 - accuracy: 0.1642\n",
      "Epoch 48/100\n",
      "433/433 [==============================] - 0s 918us/step - loss: 3.7535 - mse: 3.7535 - accuracy: 0.1608\n",
      "Epoch 49/100\n",
      "433/433 [==============================] - 0s 843us/step - loss: 3.6696 - mse: 3.6696 - accuracy: 0.1645\n",
      "Epoch 50/100\n",
      "433/433 [==============================] - 0s 878us/step - loss: 3.7509 - mse: 3.7509 - accuracy: 0.1636\n",
      "Epoch 51/100\n",
      "433/433 [==============================] - 0s 793us/step - loss: 3.6986 - mse: 3.6986 - accuracy: 0.1608\n",
      "Epoch 52/100\n",
      "433/433 [==============================] - 0s 687us/step - loss: 3.6937 - mse: 3.6937 - accuracy: 0.1653\n",
      "Epoch 53/100\n",
      "433/433 [==============================] - 0s 700us/step - loss: 3.6626 - mse: 3.6626 - accuracy: 0.1660\n",
      "Epoch 54/100\n",
      "433/433 [==============================] - 0s 697us/step - loss: 3.6534 - mse: 3.6534 - accuracy: 0.1652\n",
      "Epoch 55/100\n",
      "433/433 [==============================] - 0s 699us/step - loss: 3.5847 - mse: 3.5847 - accuracy: 0.1627\n",
      "Epoch 56/100\n",
      "433/433 [==============================] - 0s 699us/step - loss: 3.6007 - mse: 3.6007 - accuracy: 0.1640\n",
      "Epoch 57/100\n",
      "433/433 [==============================] - 0s 695us/step - loss: 3.5831 - mse: 3.5831 - accuracy: 0.1608\n",
      "Epoch 58/100\n",
      "433/433 [==============================] - 0s 701us/step - loss: 3.5042 - mse: 3.5042 - accuracy: 0.1624\n",
      "Epoch 59/100\n",
      "433/433 [==============================] - 0s 758us/step - loss: 3.5633 - mse: 3.5633 - accuracy: 0.1649\n",
      "Epoch 60/100\n",
      "433/433 [==============================] - 0s 697us/step - loss: 3.5908 - mse: 3.5908 - accuracy: 0.1627\n",
      "Epoch 61/100\n",
      "433/433 [==============================] - 0s 772us/step - loss: 3.5541 - mse: 3.5541 - accuracy: 0.1621\n",
      "Epoch 62/100\n",
      "433/433 [==============================] - 0s 742us/step - loss: 3.5371 - mse: 3.5371 - accuracy: 0.1620\n",
      "Epoch 63/100\n",
      "433/433 [==============================] - 0s 776us/step - loss: 3.5450 - mse: 3.5450 - accuracy: 0.1639\n",
      "Epoch 64/100\n",
      "433/433 [==============================] - 0s 783us/step - loss: 3.5346 - mse: 3.5346 - accuracy: 0.1651\n",
      "Epoch 65/100\n",
      "433/433 [==============================] - 0s 725us/step - loss: 3.5328 - mse: 3.5328 - accuracy: 0.1621\n",
      "Epoch 66/100\n",
      "433/433 [==============================] - 0s 718us/step - loss: 3.4755 - mse: 3.4755 - accuracy: 0.1661\n",
      "Epoch 67/100\n",
      "433/433 [==============================] - 0s 803us/step - loss: 3.5227 - mse: 3.5227 - accuracy: 0.1624\n",
      "Epoch 68/100\n",
      "433/433 [==============================] - 0s 786us/step - loss: 3.4340 - mse: 3.4340 - accuracy: 0.1643\n",
      "Epoch 69/100\n",
      "433/433 [==============================] - 0s 746us/step - loss: 3.4897 - mse: 3.4897 - accuracy: 0.1667\n",
      "Epoch 70/100\n",
      "433/433 [==============================] - 0s 821us/step - loss: 3.4735 - mse: 3.4735 - accuracy: 0.1614\n",
      "Epoch 71/100\n",
      "433/433 [==============================] - 0s 717us/step - loss: 3.4470 - mse: 3.4470 - accuracy: 0.1650\n",
      "Epoch 72/100\n",
      "433/433 [==============================] - 0s 771us/step - loss: 3.5044 - mse: 3.5044 - accuracy: 0.1629\n",
      "Epoch 73/100\n",
      "433/433 [==============================] - 0s 697us/step - loss: 3.4413 - mse: 3.4413 - accuracy: 0.1646\n",
      "Epoch 74/100\n",
      "433/433 [==============================] - 0s 701us/step - loss: 3.4495 - mse: 3.4495 - accuracy: 0.1657\n",
      "Epoch 75/100\n",
      "433/433 [==============================] - 0s 741us/step - loss: 3.4412 - mse: 3.4412 - accuracy: 0.1680\n",
      "Epoch 76/100\n",
      "433/433 [==============================] - 0s 694us/step - loss: 3.4787 - mse: 3.4787 - accuracy: 0.1635\n",
      "Epoch 77/100\n",
      "433/433 [==============================] - 0s 698us/step - loss: 3.4837 - mse: 3.4837 - accuracy: 0.1679\n",
      "Epoch 78/100\n",
      "433/433 [==============================] - 0s 716us/step - loss: 3.3992 - mse: 3.3992 - accuracy: 0.1652\n",
      "Epoch 79/100\n",
      "433/433 [==============================] - 0s 701us/step - loss: 3.4428 - mse: 3.4428 - accuracy: 0.1626\n",
      "Epoch 80/100\n",
      "433/433 [==============================] - 0s 684us/step - loss: 3.4094 - mse: 3.4094 - accuracy: 0.1655\n",
      "Epoch 81/100\n",
      "433/433 [==============================] - 0s 695us/step - loss: 3.4468 - mse: 3.4468 - accuracy: 0.1665\n",
      "Epoch 82/100\n",
      "433/433 [==============================] - 0s 707us/step - loss: 3.5102 - mse: 3.5102 - accuracy: 0.1671\n",
      "Epoch 83/100\n",
      "433/433 [==============================] - 0s 692us/step - loss: 3.4667 - mse: 3.4667 - accuracy: 0.1654\n",
      "Epoch 84/100\n",
      "433/433 [==============================] - 0s 688us/step - loss: 3.4247 - mse: 3.4247 - accuracy: 0.1659\n",
      "Epoch 85/100\n",
      "433/433 [==============================] - 0s 769us/step - loss: 3.5154 - mse: 3.5154 - accuracy: 0.1635\n",
      "Epoch 86/100\n",
      "433/433 [==============================] - 0s 697us/step - loss: 3.4502 - mse: 3.4502 - accuracy: 0.1675\n",
      "Epoch 87/100\n",
      "433/433 [==============================] - 0s 782us/step - loss: 3.3861 - mse: 3.3861 - accuracy: 0.1670\n",
      "Epoch 88/100\n",
      "433/433 [==============================] - 0s 720us/step - loss: 3.4307 - mse: 3.4307 - accuracy: 0.1656\n",
      "Epoch 89/100\n",
      "433/433 [==============================] - 0s 743us/step - loss: 3.4067 - mse: 3.4067 - accuracy: 0.1658\n",
      "Epoch 90/100\n",
      "433/433 [==============================] - 0s 731us/step - loss: 3.4544 - mse: 3.4544 - accuracy: 0.1660\n",
      "Epoch 91/100\n",
      "433/433 [==============================] - 0s 731us/step - loss: 3.4010 - mse: 3.4010 - accuracy: 0.1674\n",
      "Epoch 92/100\n",
      "433/433 [==============================] - 0s 735us/step - loss: 3.3662 - mse: 3.3662 - accuracy: 0.1679\n",
      "Epoch 93/100\n",
      "433/433 [==============================] - 0s 722us/step - loss: 3.4440 - mse: 3.4440 - accuracy: 0.1650\n",
      "Epoch 94/100\n",
      "433/433 [==============================] - 0s 725us/step - loss: 3.3981 - mse: 3.3981 - accuracy: 0.1678\n",
      "Epoch 95/100\n",
      "433/433 [==============================] - 0s 750us/step - loss: 3.3765 - mse: 3.3765 - accuracy: 0.1665\n",
      "Epoch 96/100\n",
      "433/433 [==============================] - 0s 733us/step - loss: 3.4485 - mse: 3.4485 - accuracy: 0.1621\n",
      "Epoch 97/100\n",
      "433/433 [==============================] - 0s 732us/step - loss: 3.3579 - mse: 3.3579 - accuracy: 0.1669\n",
      "Epoch 98/100\n",
      "433/433 [==============================] - 0s 739us/step - loss: 3.4044 - mse: 3.4044 - accuracy: 0.1665\n",
      "Epoch 99/100\n",
      "433/433 [==============================] - 0s 741us/step - loss: 3.4480 - mse: 3.4480 - accuracy: 0.1670\n",
      "Epoch 100/100\n",
      "433/433 [==============================] - 0s 731us/step - loss: 3.4149 - mse: 3.4149 - accuracy: 0.1666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x262d5f7aaa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 1ms/step\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 7.0, 7.0, 7.0, 7.0]\n",
      "[[ 4. ]\n",
      " [ 3.9]\n",
      " [ 3.9]\n",
      " ...\n",
      " [10.2]\n",
      " [ 9.9]\n",
      " [ 9.6]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>should</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>11.0</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>7.0</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>7.0</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is  should\n",
       "0      8.0     4.0\n",
       "1      8.0     3.9\n",
       "2      8.0     3.9\n",
       "3      8.0     3.8\n",
       "4      8.0     3.7\n",
       "...    ...     ...\n",
       "3456  11.0    10.9\n",
       "3457   7.0    10.6\n",
       "3458   7.0    10.2\n",
       "3459   7.0     9.9\n",
       "3460   7.0     9.6\n",
       "\n",
       "[3461 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 1ms/step - loss: 4.0936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.09360408782959"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(data):\n",
    "    pred = model.predict(data).flatten()\n",
    "    pred = np.rint(pred)\n",
    "    pred = pred.tolist()\n",
    "    print(pred)\n",
    "    print(y_test)\n",
    "    return pred\n",
    "\n",
    "y_pred_test = predict(X_test)\n",
    "\n",
    "df = pd.DataFrame(y_pred_test)\n",
    "df['should'] = y_test\n",
    "df.columns = ['is','should']\n",
    "display(df)\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    pred = model.predict(data).flatten()\n",
    "    pred = np.rint(pred)\n",
    "    pred = pred.tolist()\n",
    "    print(pred)\n",
    "    print(y_test)\n",
    "    return pred\n",
    "\n",
    "new_data = pd.DataFrame({'Hour': [17], 'Minute': [0], 'Weekday': [0]})\n",
    "print(model.predict(new_data).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4. ],\n",
       "       [ 3.9],\n",
       "       [ 3.9],\n",
       "       ...,\n",
       "       [10.2],\n",
       "       [ 9.9],\n",
       "       [ 9.6]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
